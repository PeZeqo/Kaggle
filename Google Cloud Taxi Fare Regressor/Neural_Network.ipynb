{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    # Delimiter lats and lons to NY only\n",
    "    df = df[(-76 <= df['pickup_longitude']) & (df['pickup_longitude'] <= -72)]\n",
    "    df = df[(-76 <= df['dropoff_longitude']) & (df['dropoff_longitude'] <= -72)]\n",
    "    df = df[(38 <= df['pickup_latitude']) & (df['pickup_latitude'] <= 42)]\n",
    "    df = df[(38 <= df['dropoff_latitude']) & (df['dropoff_latitude'] <= 42)]\n",
    "    # Remove possible outliers\n",
    "    df = df[(0 < df['fare_amount']) & (df['fare_amount'] <= 250)]\n",
    "    # Remove inconsistent values\n",
    "    df = df[(df['dropoff_longitude'] != df['pickup_longitude'])]\n",
    "    df = df[(df['dropoff_latitude'] != df['pickup_latitude'])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def late_night (row):\n",
    "    if (row['hour'] <= 6) or (row['hour'] >= 20):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def night (row):\n",
    "    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n",
    "    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n",
    "    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n",
    "    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n",
    "    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())\n",
    "    df['pickup_datetime'] =  df['pickup_datetime'].apply(lambda x: str(x))\n",
    "    df['night'] = df.apply (lambda x: night(x), axis=1)\n",
    "    df['late_night'] = df.apply (lambda x: late_night(x), axis=1)\n",
    "    # Drop 'pickup_datetime' as we won't need it anymore\n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_distance_features(df):\n",
    "    df['horizontal_dist'] = df[\"pickup_longitude\"] - df[\"dropoff_longitude\"]\n",
    "    df['vertical_dist'] = df[\"pickup_latitude\"] - df[\"dropoff_latitude\"]\n",
    "    df['euclidian'] = df['horizontal_dist'] ** 2 + df['vertical_dist'] ** 2\n",
    "    \n",
    "    ny_coord = (40.7141667, -74.0063889)\n",
    "    jfk_coord = (40.639722, -73.778889)\n",
    "    ewr_coord = (40.6925, -74.168611)\n",
    "    lga_coord = (40.77725, -73.872611)\n",
    "    \n",
    "    df['jfk_pickup'] = ((df[\"pickup_longitude\"] - jfk_coord[1]) ** 2 + (df[\"pickup_latitude\"] - jfk_coord[0]) ** 2) ** (1/2)\n",
    "    df['jfk_dropoff'] = ((df[\"dropoff_longitude\"] - jfk_coord[1]) ** 2 + (df[\"dropoff_latitude\"] - jfk_coord[0]) ** 2) ** (1/2)\n",
    "    \n",
    "    df['ewr_pickup'] = ((df[\"pickup_longitude\"] - ewr_coord[1]) ** 2 + (df[\"pickup_latitude\"] - ewr_coord[0]) ** 2) ** (1/2)\n",
    "    df['ewr_dropoff'] = ((df[\"dropoff_longitude\"] - ewr_coord[1]) ** 2 + (df[\"dropoff_latitude\"] - ewr_coord[0]) ** 2) ** (1/2)\n",
    "    \n",
    "    df['lga_pickup'] = ((df[\"pickup_longitude\"] - lga_coord[1]) ** 2 + (df[\"pickup_latitude\"] - lga_coord[0]) ** 2) ** (1/2)\n",
    "    df['lga_dropoff'] = ((df[\"dropoff_longitude\"] - lga_coord[1]) ** 2 + (df[\"dropoff_latitude\"] - lga_coord[0]) ** 2) ** (1/2)\n",
    "    \n",
    "    df['lga_pickup'] = ((df[\"pickup_longitude\"] - ny_coord[1]) ** 2 + (df[\"pickup_latitude\"] - ny_coord[0]) ** 2) ** (1/2)\n",
    "    df['lga_dropoff'] = ((df[\"dropoff_longitude\"] - ny_coord[1]) ** 2 + (df[\"dropoff_latitude\"] - ny_coord[0]) ** 2) ** (1/2)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(history.history['root_mean_squared_error'])\n",
    "    plt.plot(history.history['val_root_mean_squared_error'])\n",
    "    plt.title('model MSE')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_submission(raw_test, prediction, id_column, prediction_column, file_name):\n",
    "    df = pd.DataFrame(prediction, columns=[prediction_column])\n",
    "    df[id_column] = raw_test[id_column]\n",
    "    df[[id_column, prediction_column]].to_csv((file_name), index=False)\n",
    "    print('Output complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes = {'key': 'str', \n",
    "              'fare_amount': 'float32',\n",
    "              'pickup_datetime': 'str', \n",
    "              'pickup_longitude': 'float32',\n",
    "              'pickup_latitude': 'float32',\n",
    "              'dropoff_longitude': 'float32',\n",
    "              'dropoff_latitude': 'float32',\n",
    "              'passenger_count': 'uint8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 5000000\n",
    "perc = 0.5\n",
    "\n",
    "\n",
    "# train_full = pd.read_csv('train.csv', nrows=DATA_SIZE, dtype=datatypes, usecols=[1,2,3,4,5,6,7])\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# train = train_full.sample(frac=perc)\n",
    "\n",
    "# train = clean(train)\n",
    "\n",
    "# train_clean = add_time_features(train)\n",
    "# test_clean = add_time_features(test)\n",
    "\n",
    "# train_clean = add_distance_features(train_clean)\n",
    "# test_clean = add_distance_features(test_clean)\n",
    "\n",
    "# dropped_columns = ['pickup_longitude', 'pickup_latitude', \n",
    "#                    'dropoff_longitude', 'dropoff_latitude',\n",
    "#                    'horizontal_dist', 'vertical_dist']\n",
    "# train_clean = train_clean.drop(dropped_columns, axis=1)\n",
    "# test_clean = test_clean.drop(dropped_columns + ['key'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clean.to_csv('train_clean.csv')\n",
    "# test_clean.to_csv('test_clean.csv')\n",
    "\n",
    "train_clean = pd.read_csv('train_clean.csv')\n",
    "train_clean = train_clean.sample(frac=perc)\n",
    "\n",
    "test_clean = pd.read_csv('test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>night</th>\n",
       "      <th>late_night</th>\n",
       "      <th>euclidian</th>\n",
       "      <th>jfk_pickup</th>\n",
       "      <th>jfk_dropoff</th>\n",
       "      <th>ewr_pickup</th>\n",
       "      <th>ewr_dropoff</th>\n",
       "      <th>lga_pickup</th>\n",
       "      <th>lga_dropoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1165548</th>\n",
       "      <td>206120</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.227444</td>\n",
       "      <td>0.225258</td>\n",
       "      <td>0.215074</td>\n",
       "      <td>0.231573</td>\n",
       "      <td>0.066285</td>\n",
       "      <td>0.084417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778287</th>\n",
       "      <td>2085120</td>\n",
       "      <td>23.70</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>0.226376</td>\n",
       "      <td>0.240935</td>\n",
       "      <td>0.202674</td>\n",
       "      <td>0.289992</td>\n",
       "      <td>0.049630</td>\n",
       "      <td>0.151811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584278</th>\n",
       "      <td>622618</td>\n",
       "      <td>10.10</td>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.237349</td>\n",
       "      <td>0.238439</td>\n",
       "      <td>0.201543</td>\n",
       "      <td>0.191710</td>\n",
       "      <td>0.056964</td>\n",
       "      <td>0.045242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820325</th>\n",
       "      <td>705415</td>\n",
       "      <td>6.90</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.250153</td>\n",
       "      <td>0.239601</td>\n",
       "      <td>0.167781</td>\n",
       "      <td>0.167590</td>\n",
       "      <td>0.026375</td>\n",
       "      <td>0.008761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829002</th>\n",
       "      <td>4893779</td>\n",
       "      <td>27.47</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.154118</td>\n",
       "      <td>0.231541</td>\n",
       "      <td>0.315344</td>\n",
       "      <td>0.190262</td>\n",
       "      <td>0.153815</td>\n",
       "      <td>0.036587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  fare_amount  passenger_count  year  month  day  hour  \\\n",
       "1165548      206120         5.30                1  2009     11   24    20   \n",
       "4778287     2085120        23.70                1  2012      6   18     8   \n",
       "3584278      622618        10.10                4  2011      9    9    10   \n",
       "2820325      705415         6.90                1  2011      3   22    23   \n",
       "3829002     4893779        27.47                1  2009      8   25    11   \n",
       "\n",
       "         weekday  night  late_night  euclidian  jfk_pickup  jfk_dropoff  \\\n",
       "1165548        1      1           1   0.000333    0.227444     0.225258   \n",
       "4778287        0      0           0   0.010464    0.226376     0.240935   \n",
       "3584278        4      0           0   0.000141    0.237349     0.238439   \n",
       "2820325        1      0           1   0.000340    0.250153     0.239601   \n",
       "3829002        1      0           0   0.015700    0.154118     0.231541   \n",
       "\n",
       "         ewr_pickup  ewr_dropoff  lga_pickup  lga_dropoff  \n",
       "1165548    0.215074     0.231573    0.066285     0.084417  \n",
       "4778287    0.202674     0.289992    0.049630     0.151811  \n",
       "3584278    0.201543     0.191710    0.056964     0.045242  \n",
       "2820325    0.167781     0.167590    0.026375     0.008761  \n",
       "3829002    0.315344     0.190262    0.153815     0.036587  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>night</th>\n",
       "      <th>late_night</th>\n",
       "      <th>euclidian</th>\n",
       "      <th>jfk_pickup</th>\n",
       "      <th>jfk_dropoff</th>\n",
       "      <th>ewr_pickup</th>\n",
       "      <th>ewr_dropoff</th>\n",
       "      <th>lga_pickup</th>\n",
       "      <th>lga_dropoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.230651</td>\n",
       "      <td>0.227733</td>\n",
       "      <td>0.207901</td>\n",
       "      <td>0.194093</td>\n",
       "      <td>0.059645</td>\n",
       "      <td>0.038771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.222708</td>\n",
       "      <td>0.241443</td>\n",
       "      <td>0.183726</td>\n",
       "      <td>0.176033</td>\n",
       "      <td>0.020212</td>\n",
       "      <td>0.026134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.232181</td>\n",
       "      <td>0.227225</td>\n",
       "      <td>0.195144</td>\n",
       "      <td>0.196423</td>\n",
       "      <td>0.044107</td>\n",
       "      <td>0.041677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.239415</td>\n",
       "      <td>0.239336</td>\n",
       "      <td>0.202012</td>\n",
       "      <td>0.187721</td>\n",
       "      <td>0.059277</td>\n",
       "      <td>0.040718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.239883</td>\n",
       "      <td>0.234365</td>\n",
       "      <td>0.224711</td>\n",
       "      <td>0.187385</td>\n",
       "      <td>0.085698</td>\n",
       "      <td>0.035119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  passenger_count  year  month  day  hour  weekday  night  \\\n",
       "0           0                1  2015      1   27    13        1      0   \n",
       "1           1                1  2015      1   27    13        1      0   \n",
       "2           2                1  2011     10    8    11        5      0   \n",
       "3           3                1  2012     12    1    21        5      0   \n",
       "4           4                1  2012     12    1    21        5      0   \n",
       "\n",
       "   late_night  euclidian  jfk_pickup  jfk_dropoff  ewr_pickup  ewr_dropoff  \\\n",
       "0           0   0.000465    0.230651     0.227733    0.207901     0.194093   \n",
       "1           0   0.000537    0.222708     0.241443    0.183726     0.176033   \n",
       "2           0   0.000034    0.232181     0.227225    0.195144     0.196423   \n",
       "3           1   0.000348    0.239415     0.239336    0.202012     0.187721   \n",
       "4           1   0.002564    0.239883     0.234365    0.224711     0.187385   \n",
       "\n",
       "   lga_pickup  lga_dropoff  \n",
       "0    0.059645     0.038771  \n",
       "1    0.020212     0.026134  \n",
       "2    0.044107     0.041677  \n",
       "3    0.059277     0.040718  \n",
       "4    0.085698     0.035119  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(train_clean, test_size=0.2, random_state=1)\n",
    "\n",
    "train_labels = train_df['fare_amount'].values\n",
    "validation_labels = validation_df['fare_amount'].values\n",
    "train_df = train_df.drop(['fare_amount'], axis=1)\n",
    "validation_df = validation_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_df_scaled = scaler.fit_transform(train_df)\n",
    "validation_df_scaled = scaler.transform(validation_df)\n",
    "test_scaled = scaler.transform(test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_scheduale(epoch, learning_rate):\n",
    "#     if epoch % 20 == 0:\n",
    "#         numInt = epoch / 20\n",
    "#         if numInt == 0:\n",
    "#             return learning_rate\n",
    "#         return learning_rate * ((1/4) ** (1/numInt))\n",
    "    if epoch == 20:\n",
    "        return learning_rate * (1/4)\n",
    "    if epoch == 35:\n",
    "        return learning_rate * (1/2)\n",
    "    return learning_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(LEARNING_RATE, REGULARIZATION_RATE, layers, opt_choice):\n",
    "    model = Sequential()\n",
    "    first = True\n",
    "    for layer in layers:\n",
    "        if   first:\n",
    "            model.add(Dense(layer, activation='relu', input_dim=train_df_scaled.shape[1], \n",
    "                            activity_regularizer=regularizers.l1_l2(REGULARIZATION_RATE, REGULARIZATION_RATE)))\n",
    "            model.add(BatchNormalization())\n",
    "            first = False\n",
    "        else:\n",
    "            model.add(Dense(layer, activation='relu'))\n",
    "            model.add(BatchNormalization())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    if opt_choice == 0:\n",
    "        opt = optimizers.Adam(lr=LEARNING_RATE)\n",
    "    elif opt_choice == 1:\n",
    "        opt = optimizers.RMSprop(lr=LEARNING_RATE)\n",
    "    elif opt_choice == 2:\n",
    "        opt = optimizers.Adagrad(lr=LEARNING_RATE)\n",
    "    else:\n",
    "        opt = optimizers.SGD(lr=LEARNING_RATE)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=[root_mean_squared_error])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1935368 samples, validate on 483842 samples\n",
      "Epoch 1/50\n",
      "1935368/1935368 [==============================] - 53s 27us/step - loss: 13.0273 - root_mean_squared_error: 3.4385 - val_loss: 13.3539 - val_root_mean_squared_error: 3.4691\n",
      "Epoch 2/50\n",
      "1935368/1935368 [==============================] - 44s 23us/step - loss: 12.8341 - root_mean_squared_error: 3.4139 - val_loss: 13.2218 - val_root_mean_squared_error: 3.4485\n",
      "Epoch 3/50\n",
      "1935368/1935368 [==============================] - 44s 23us/step - loss: 12.7184 - root_mean_squared_error: 3.4009 - val_loss: 13.2264 - val_root_mean_squared_error: 3.4495\n",
      "Epoch 4/50\n",
      "1935368/1935368 [==============================] - 45s 23us/step - loss: 12.6004 - root_mean_squared_error: 3.3836 - val_loss: 15.2614 - val_root_mean_squared_error: 3.7472\n",
      "Epoch 5/50\n",
      " 438784/1935368 [=====>........................] - ETA: 34s - loss: 12.9422 - root_mean_squared_error: 3.430"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "REGULARIZATION_RATE = 0.01\n",
    "make_new_model = True\n",
    "\n",
    "callback = [#EarlyStopping(patience=60, monitor='val_loss'),\n",
    "            #ReduceLROnPlateau(patience=20, monitor='val_loss', factor=0.5, min_lr=0.00001, verbose=1),\n",
    "            LearningRateScheduler(learning_scheduale),\n",
    "            ModelCheckpoint('model', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "if make_new_model:\n",
    "    model = make_model(LEARNING_RATE, REGULARIZATION_RATE, [256] * 6, 0)\n",
    "    model.load_weights('model')\n",
    "\n",
    "history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    verbose=1, validation_data=(validation_df_scaled, validation_labels), \n",
    "                    shuffle=True, callbacks = callback)\n",
    "\n",
    "#model.save_weights('model_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.462903876566608"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_scaled, batch_size=512, verbose=1)\n",
    "output_submission(test, prediction, 'key', 'fare_amount', 'submission_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9914/9914 [==============================] - 1s 114us/step\n",
      "Output complete\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model')\n",
    "\n",
    "prediction = model.predict(test_scaled, batch_size=512, verbose=1)\n",
    "output_submission(test, prediction, 'key', 'fare_amount', 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.001\n",
      "\n",
      "Train on 193536 samples, validate on 48385 samples\n",
      "Epoch 1/50\n",
      "193536/193536 [==============================] - 10s 50us/step - loss: 88.8388 - root_mean_squared_error: 8.3706 - val_loss: 90.2425 - val_root_mean_squared_error: 9.3091\n",
      "Epoch 2/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 28.6618 - root_mean_squared_error: 5.1233 - val_loss: 29.5053 - val_root_mean_squared_error: 5.1979\n",
      "Epoch 3/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 22.8947 - root_mean_squared_error: 4.6253 - val_loss: 29.5364 - val_root_mean_squared_error: 5.3376\n",
      "Epoch 4/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 19.8728 - root_mean_squared_error: 4.2970 - val_loss: 23.4289 - val_root_mean_squared_error: 4.7148\n",
      "Epoch 5/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 18.2234 - root_mean_squared_error: 4.1110 - val_loss: 21.6128 - val_root_mean_squared_error: 4.5057\n",
      "Epoch 6/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 16.9860 - root_mean_squared_error: 3.9599 - val_loss: 25.9502 - val_root_mean_squared_error: 4.9374\n",
      "Epoch 7/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 16.5497 - root_mean_squared_error: 3.9076 - val_loss: 43.2668 - val_root_mean_squared_error: 6.4898\n",
      "Epoch 8/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 16.1208 - root_mean_squared_error: 3.8516 - val_loss: 17.6458 - val_root_mean_squared_error: 4.0199\n",
      "Epoch 9/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 15.9054 - root_mean_squared_error: 3.8231 - val_loss: 20.7206 - val_root_mean_squared_error: 4.3688\n",
      "Epoch 10/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 15.7696 - root_mean_squared_error: 3.8027 - val_loss: 23.2181 - val_root_mean_squared_error: 4.6471\n",
      "Epoch 11/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 15.6979 - root_mean_squared_error: 3.8047 - val_loss: 23.5100 - val_root_mean_squared_error: 4.7154\n",
      "Epoch 12/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 15.3874 - root_mean_squared_error: 3.7495 - val_loss: 24.9018 - val_root_mean_squared_error: 4.8596\n",
      "Epoch 13/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 15.0660 - root_mean_squared_error: 3.7243 - val_loss: 17.9273 - val_root_mean_squared_error: 4.0113\n",
      "Epoch 14/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 14.7864 - root_mean_squared_error: 3.6841 - val_loss: 33.1497 - val_root_mean_squared_error: 5.6285\n",
      "Epoch 15/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 14.9209 - root_mean_squared_error: 3.7076 - val_loss: 20.9949 - val_root_mean_squared_error: 4.3519\n",
      "Epoch 16/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 14.6069 - root_mean_squared_error: 3.6654 - val_loss: 191.8474 - val_root_mean_squared_error: 13.8159\n",
      "Epoch 17/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 14.5850 - root_mean_squared_error: 3.6615 - val_loss: 17.2411 - val_root_mean_squared_error: 3.9937\n",
      "Epoch 18/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 14.1754 - root_mean_squared_error: 3.6102 - val_loss: 23.1900 - val_root_mean_squared_error: 4.4323\n",
      "Epoch 19/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 14.2920 - root_mean_squared_error: 3.6166 - val_loss: 28.7691 - val_root_mean_squared_error: 5.2210\n",
      "Epoch 20/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 14.0082 - root_mean_squared_error: 3.5898 - val_loss: 24.4676 - val_root_mean_squared_error: 4.7772\n",
      "Epoch 21/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 14.0918 - root_mean_squared_error: 3.5940 - val_loss: 28.1411 - val_root_mean_squared_error: 5.1999\n",
      "Epoch 22/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 14.1866 - root_mean_squared_error: 3.5953 - val_loss: 20.6071 - val_root_mean_squared_error: 4.2492\n",
      "Epoch 23/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 14.3271 - root_mean_squared_error: 3.6103 - val_loss: 19.1351 - val_root_mean_squared_error: 4.1507\n",
      "Epoch 24/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 14.1928 - root_mean_squared_error: 3.6080 - val_loss: 64.1339 - val_root_mean_squared_error: 7.9368\n",
      "Epoch 25/50\n",
      "193536/193536 [==============================] - 4s 23us/step - loss: 14.0035 - root_mean_squared_error: 3.5723 - val_loss: 19.4362 - val_root_mean_squared_error: 4.2651\n",
      "Epoch 26/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 14.1167 - root_mean_squared_error: 3.5964 - val_loss: 18.7140 - val_root_mean_squared_error: 4.1684\n",
      "Epoch 27/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.9473 - root_mean_squared_error: 3.5622 - val_loss: 18.5199 - val_root_mean_squared_error: 4.1189\n",
      "Epoch 28/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 13.8631 - root_mean_squared_error: 3.5673 - val_loss: 17.8593 - val_root_mean_squared_error: 3.9997\n",
      "Epoch 29/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 13.4350 - root_mean_squared_error: 3.5107 - val_loss: 49.9371 - val_root_mean_squared_error: 6.9806\n",
      "Epoch 30/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.6867 - root_mean_squared_error: 3.5273 - val_loss: 19.2939 - val_root_mean_squared_error: 4.1653\n",
      "Epoch 31/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.4168 - root_mean_squared_error: 3.4963 - val_loss: 19.1846 - val_root_mean_squared_error: 4.2401\n",
      "Epoch 32/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.6683 - root_mean_squared_error: 3.5287 - val_loss: 28.8475 - val_root_mean_squared_error: 5.0739\n",
      "Epoch 33/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.8149 - root_mean_squared_error: 3.5425 - val_loss: 19.5902 - val_root_mean_squared_error: 4.1319\n",
      "Epoch 34/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.1096 - root_mean_squared_error: 3.4676 - val_loss: 17.0357 - val_root_mean_squared_error: 3.9527\n",
      "Epoch 35/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.5007 - root_mean_squared_error: 3.5039 - val_loss: 21.4341 - val_root_mean_squared_error: 4.3560\n",
      "Epoch 36/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.2536 - root_mean_squared_error: 3.4749 - val_loss: 53.0567 - val_root_mean_squared_error: 7.1083\n",
      "Epoch 37/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.4606 - root_mean_squared_error: 3.5090 - val_loss: 19.6002 - val_root_mean_squared_error: 4.2224\n",
      "Epoch 38/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.3058 - root_mean_squared_error: 3.4865 - val_loss: 21.0601 - val_root_mean_squared_error: 4.3738\n",
      "Epoch 39/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 12.9796 - root_mean_squared_error: 3.4299 - val_loss: 20.1202 - val_root_mean_squared_error: 4.2429\n",
      "Epoch 40/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 12.9355 - root_mean_squared_error: 3.4396 - val_loss: 18.7391 - val_root_mean_squared_error: 4.1202\n",
      "Epoch 41/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.1190 - root_mean_squared_error: 3.4515 - val_loss: 18.8576 - val_root_mean_squared_error: 4.0603\n",
      "Epoch 42/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 12.7547 - root_mean_squared_error: 3.4050 - val_loss: 23.3859 - val_root_mean_squared_error: 4.6849\n",
      "Epoch 43/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.2645 - root_mean_squared_error: 3.4733 - val_loss: 29.5830 - val_root_mean_squared_error: 5.3310\n",
      "Epoch 44/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 12.8971 - root_mean_squared_error: 3.4283 - val_loss: 19.7477 - val_root_mean_squared_error: 4.2605\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193536/193536 [==============================] - 4s 20us/step - loss: 12.8923 - root_mean_squared_error: 3.4249 - val_loss: 23.4272 - val_root_mean_squared_error: 4.6999\n",
      "Epoch 46/50\n",
      "193536/193536 [==============================] - 4s 20us/step - loss: 13.2145 - root_mean_squared_error: 3.4613 - val_loss: 18.2791 - val_root_mean_squared_error: 4.0701\n",
      "Epoch 47/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 13.0122 - root_mean_squared_error: 3.4347 - val_loss: 257.4138 - val_root_mean_squared_error: 16.0063\n",
      "Epoch 48/50\n",
      "193536/193536 [==============================] - 5s 25us/step - loss: 12.7988 - root_mean_squared_error: 3.4140 - val_loss: 14.8258 - val_root_mean_squared_error: 3.6497\n",
      "Epoch 49/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 12.7853 - root_mean_squared_error: 3.4058 - val_loss: 38.8264 - val_root_mean_squared_error: 6.1501\n",
      "Epoch 50/50\n",
      "193536/193536 [==============================] - 5s 25us/step - loss: 12.7096 - root_mean_squared_error: 3.3973 - val_loss: 227.7568 - val_root_mean_squared_error: 15.0578\n",
      "\n",
      "Learning Rate: 0.00025\n",
      "\n",
      "Train on 193536 samples, validate on 48385 samples\n",
      "Epoch 1/50\n",
      "193536/193536 [==============================] - 11s 55us/step - loss: 11.7884 - root_mean_squared_error: 3.2766 - val_loss: 14.5410 - val_root_mean_squared_error: 3.6275\n",
      "Epoch 2/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.5258 - root_mean_squared_error: 3.2442 - val_loss: 17.5032 - val_root_mean_squared_error: 4.0325\n",
      "Epoch 3/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.4595 - root_mean_squared_error: 3.2255 - val_loss: 16.0631 - val_root_mean_squared_error: 3.8353\n",
      "Epoch 4/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.4154 - root_mean_squared_error: 3.2205 - val_loss: 14.7403 - val_root_mean_squared_error: 3.6334\n",
      "Epoch 5/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.3081 - root_mean_squared_error: 3.2142 - val_loss: 16.9171 - val_root_mean_squared_error: 3.9572\n",
      "Epoch 6/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.3125 - root_mean_squared_error: 3.2094 - val_loss: 13.9794 - val_root_mean_squared_error: 3.5390\n",
      "Epoch 7/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.2744 - root_mean_squared_error: 3.1985 - val_loss: 14.8985 - val_root_mean_squared_error: 3.6722\n",
      "Epoch 8/50\n",
      "193536/193536 [==============================] - 5s 24us/step - loss: 11.2474 - root_mean_squared_error: 3.1975 - val_loss: 13.7560 - val_root_mean_squared_error: 3.5245\n",
      "Epoch 9/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 11.1894 - root_mean_squared_error: 3.1939 - val_loss: 14.0856 - val_root_mean_squared_error: 3.5713\n",
      "Epoch 10/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.1328 - root_mean_squared_error: 3.1797 - val_loss: 17.0764 - val_root_mean_squared_error: 3.9753\n",
      "Epoch 11/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.1110 - root_mean_squared_error: 3.1871 - val_loss: 15.5719 - val_root_mean_squared_error: 3.7892\n",
      "Epoch 12/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.0627 - root_mean_squared_error: 3.1807 - val_loss: 42.0364 - val_root_mean_squared_error: 6.4229\n",
      "Epoch 13/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.1584 - root_mean_squared_error: 3.1906 - val_loss: 15.9241 - val_root_mean_squared_error: 3.8349\n",
      "Epoch 14/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.0340 - root_mean_squared_error: 3.1683 - val_loss: 13.8401 - val_root_mean_squared_error: 3.5356\n",
      "Epoch 15/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.0117 - root_mean_squared_error: 3.1764 - val_loss: 14.6118 - val_root_mean_squared_error: 3.6499\n",
      "Epoch 16/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.9897 - root_mean_squared_error: 3.1666 - val_loss: 14.4019 - val_root_mean_squared_error: 3.6142\n",
      "Epoch 17/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.9297 - root_mean_squared_error: 3.1539 - val_loss: 14.3946 - val_root_mean_squared_error: 3.6169\n",
      "Epoch 18/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.8820 - root_mean_squared_error: 3.1461 - val_loss: 13.5527 - val_root_mean_squared_error: 3.4935\n",
      "Epoch 19/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.8161 - root_mean_squared_error: 3.1365 - val_loss: 15.4412 - val_root_mean_squared_error: 3.7696\n",
      "Epoch 20/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.9438 - root_mean_squared_error: 3.1534 - val_loss: 21.4230 - val_root_mean_squared_error: 4.5157\n",
      "Epoch 21/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.8187 - root_mean_squared_error: 3.1400 - val_loss: 16.1688 - val_root_mean_squared_error: 3.8589\n",
      "Epoch 22/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.8861 - root_mean_squared_error: 3.1483 - val_loss: 14.0354 - val_root_mean_squared_error: 3.5675\n",
      "Epoch 23/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.7963 - root_mean_squared_error: 3.1455 - val_loss: 14.8464 - val_root_mean_squared_error: 3.6866\n",
      "Epoch 24/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.8469 - root_mean_squared_error: 3.1442 - val_loss: 14.6845 - val_root_mean_squared_error: 3.6612\n",
      "Epoch 25/50\n",
      "193536/193536 [==============================] - 5s 26us/step - loss: 10.8361 - root_mean_squared_error: 3.1407 - val_loss: 14.1601 - val_root_mean_squared_error: 3.5847\n",
      "Epoch 26/50\n",
      "193536/193536 [==============================] - 4s 23us/step - loss: 10.7944 - root_mean_squared_error: 3.1359 - val_loss: 13.8947 - val_root_mean_squared_error: 3.5499\n",
      "Epoch 27/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.9119 - root_mean_squared_error: 3.1505 - val_loss: 15.5987 - val_root_mean_squared_error: 3.7928\n",
      "Epoch 28/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.7098 - root_mean_squared_error: 3.1245 - val_loss: 15.7217 - val_root_mean_squared_error: 3.8131\n",
      "Epoch 29/50\n",
      "193536/193536 [==============================] - 5s 26us/step - loss: 10.7011 - root_mean_squared_error: 3.1309 - val_loss: 14.0166 - val_root_mean_squared_error: 3.5636\n",
      "Epoch 30/50\n",
      "193536/193536 [==============================] - 5s 26us/step - loss: 10.6974 - root_mean_squared_error: 3.1220 - val_loss: 14.4053 - val_root_mean_squared_error: 3.6177\n",
      "Epoch 31/50\n",
      "193536/193536 [==============================] - 5s 24us/step - loss: 10.6603 - root_mean_squared_error: 3.1175 - val_loss: 15.9730 - val_root_mean_squared_error: 3.8393\n",
      "Epoch 32/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.7755 - root_mean_squared_error: 3.1333 - val_loss: 20.5182 - val_root_mean_squared_error: 4.4065\n",
      "Epoch 33/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.7371 - root_mean_squared_error: 3.1241 - val_loss: 15.6497 - val_root_mean_squared_error: 3.7978\n",
      "Epoch 34/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.7099 - root_mean_squared_error: 3.1238 - val_loss: 18.1482 - val_root_mean_squared_error: 4.1207\n",
      "Epoch 35/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6148 - root_mean_squared_error: 3.1083 - val_loss: 14.1267 - val_root_mean_squared_error: 3.5796\n",
      "Epoch 36/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5606 - root_mean_squared_error: 3.1031 - val_loss: 14.3138 - val_root_mean_squared_error: 3.5954\n",
      "Epoch 37/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6793 - root_mean_squared_error: 3.1122 - val_loss: 15.7259 - val_root_mean_squared_error: 3.7903\n",
      "Epoch 38/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5101 - root_mean_squared_error: 3.0978 - val_loss: 23.3627 - val_root_mean_squared_error: 4.7236\n",
      "Epoch 39/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.9311 - root_mean_squared_error: 3.1619 - val_loss: 13.8382 - val_root_mean_squared_error: 3.5307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6304 - root_mean_squared_error: 3.1128 - val_loss: 17.8545 - val_root_mean_squared_error: 4.0787\n",
      "Epoch 41/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 11.0233 - root_mean_squared_error: 3.1689 - val_loss: 20.9330 - val_root_mean_squared_error: 4.4487\n",
      "Epoch 42/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4653 - root_mean_squared_error: 3.0830 - val_loss: 13.9670 - val_root_mean_squared_error: 3.5454\n",
      "Epoch 43/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4447 - root_mean_squared_error: 3.0901 - val_loss: 13.8482 - val_root_mean_squared_error: 3.5332\n",
      "Epoch 44/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4173 - root_mean_squared_error: 3.0864 - val_loss: 15.3665 - val_root_mean_squared_error: 3.7635\n",
      "Epoch 45/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6335 - root_mean_squared_error: 3.1081 - val_loss: 19.7578 - val_root_mean_squared_error: 4.3015\n",
      "Epoch 46/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5980 - root_mean_squared_error: 3.1061 - val_loss: 14.4667 - val_root_mean_squared_error: 3.6165\n",
      "Epoch 47/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3938 - root_mean_squared_error: 3.0798 - val_loss: 19.6957 - val_root_mean_squared_error: 4.2917\n",
      "Epoch 48/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6725 - root_mean_squared_error: 3.1144 - val_loss: 14.0931 - val_root_mean_squared_error: 3.5714\n",
      "Epoch 49/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3667 - root_mean_squared_error: 3.0712 - val_loss: 14.3181 - val_root_mean_squared_error: 3.5945\n",
      "Epoch 50/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3344 - root_mean_squared_error: 3.0724 - val_loss: 18.2003 - val_root_mean_squared_error: 4.1233\n",
      "\n",
      "Learning Rate: 0.00015\n",
      "\n",
      "Train on 193536 samples, validate on 48385 samples\n",
      "Epoch 1/50\n",
      "193536/193536 [==============================] - 11s 56us/step - loss: 10.7041 - root_mean_squared_error: 3.1294 - val_loss: 14.6684 - val_root_mean_squared_error: 3.6491\n",
      "Epoch 2/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6753 - root_mean_squared_error: 3.1159 - val_loss: 13.9678 - val_root_mean_squared_error: 3.5577\n",
      "Epoch 3/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.7284 - root_mean_squared_error: 3.1223 - val_loss: 13.7724 - val_root_mean_squared_error: 3.5241\n",
      "Epoch 4/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.7280 - root_mean_squared_error: 3.1208 - val_loss: 13.5342 - val_root_mean_squared_error: 3.5005\n",
      "Epoch 5/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6773 - root_mean_squared_error: 3.1218 - val_loss: 14.5299 - val_root_mean_squared_error: 3.6427\n",
      "Epoch 6/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6069 - root_mean_squared_error: 3.1066 - val_loss: 14.1505 - val_root_mean_squared_error: 3.5829\n",
      "Epoch 7/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6243 - root_mean_squared_error: 3.1150 - val_loss: 13.6128 - val_root_mean_squared_error: 3.5102\n",
      "Epoch 8/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5592 - root_mean_squared_error: 3.1064 - val_loss: 13.5801 - val_root_mean_squared_error: 3.5006\n",
      "Epoch 9/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5398 - root_mean_squared_error: 3.1094 - val_loss: 13.7491 - val_root_mean_squared_error: 3.5164\n",
      "Epoch 10/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6997 - root_mean_squared_error: 3.1245 - val_loss: 15.5059 - val_root_mean_squared_error: 3.7781\n",
      "Epoch 11/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5589 - root_mean_squared_error: 3.1013 - val_loss: 13.7021 - val_root_mean_squared_error: 3.5090\n",
      "Epoch 12/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5067 - root_mean_squared_error: 3.0887 - val_loss: 14.3336 - val_root_mean_squared_error: 3.6091\n",
      "Epoch 13/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4619 - root_mean_squared_error: 3.0955 - val_loss: 14.5966 - val_root_mean_squared_error: 3.6486\n",
      "Epoch 14/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5922 - root_mean_squared_error: 3.1115 - val_loss: 15.4330 - val_root_mean_squared_error: 3.7513\n",
      "Epoch 15/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4743 - root_mean_squared_error: 3.0858 - val_loss: 15.6497 - val_root_mean_squared_error: 3.7976\n",
      "Epoch 16/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4788 - root_mean_squared_error: 3.0977 - val_loss: 13.7654 - val_root_mean_squared_error: 3.5295\n",
      "Epoch 17/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5715 - root_mean_squared_error: 3.1121 - val_loss: 15.1941 - val_root_mean_squared_error: 3.7271\n",
      "Epoch 18/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4584 - root_mean_squared_error: 3.0823 - val_loss: 13.9722 - val_root_mean_squared_error: 3.5578\n",
      "Epoch 19/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3536 - root_mean_squared_error: 3.0726 - val_loss: 13.7541 - val_root_mean_squared_error: 3.5336\n",
      "Epoch 20/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4773 - root_mean_squared_error: 3.0920 - val_loss: 14.6041 - val_root_mean_squared_error: 3.6519\n",
      "Epoch 21/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5237 - root_mean_squared_error: 3.1006 - val_loss: 14.2757 - val_root_mean_squared_error: 3.6086\n",
      "Epoch 22/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3421 - root_mean_squared_error: 3.0686 - val_loss: 13.9781 - val_root_mean_squared_error: 3.5529\n",
      "Epoch 23/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3265 - root_mean_squared_error: 3.0757 - val_loss: 14.3489 - val_root_mean_squared_error: 3.6150\n",
      "Epoch 24/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4089 - root_mean_squared_error: 3.0805 - val_loss: 14.2052 - val_root_mean_squared_error: 3.5925\n",
      "Epoch 25/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2901 - root_mean_squared_error: 3.0659 - val_loss: 14.6525 - val_root_mean_squared_error: 3.6555\n",
      "Epoch 26/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3954 - root_mean_squared_error: 3.0848 - val_loss: 13.6223 - val_root_mean_squared_error: 3.5117\n",
      "Epoch 27/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3260 - root_mean_squared_error: 3.0634 - val_loss: 14.2070 - val_root_mean_squared_error: 3.5945\n",
      "Epoch 28/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3053 - root_mean_squared_error: 3.0745 - val_loss: 14.2866 - val_root_mean_squared_error: 3.6020\n",
      "Epoch 29/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.3235 - root_mean_squared_error: 3.0678 - val_loss: 14.5122 - val_root_mean_squared_error: 3.6288\n",
      "Epoch 30/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2302 - root_mean_squared_error: 3.0594 - val_loss: 14.3019 - val_root_mean_squared_error: 3.6042\n",
      "Epoch 31/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3298 - root_mean_squared_error: 3.0740 - val_loss: 14.1899 - val_root_mean_squared_error: 3.5957\n",
      "Epoch 32/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.6587 - root_mean_squared_error: 3.1161 - val_loss: 16.1425 - val_root_mean_squared_error: 3.8645\n",
      "Epoch 33/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2311 - root_mean_squared_error: 3.0573 - val_loss: 14.1895 - val_root_mean_squared_error: 3.5922\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2112 - root_mean_squared_error: 3.0416 - val_loss: 14.3841 - val_root_mean_squared_error: 3.6180\n",
      "Epoch 35/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1944 - root_mean_squared_error: 3.0498 - val_loss: 14.0740 - val_root_mean_squared_error: 3.5726\n",
      "Epoch 36/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1641 - root_mean_squared_error: 3.0496 - val_loss: 14.6157 - val_root_mean_squared_error: 3.6437\n",
      "Epoch 37/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2765 - root_mean_squared_error: 3.0651 - val_loss: 14.0058 - val_root_mean_squared_error: 3.5627\n",
      "Epoch 38/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2863 - root_mean_squared_error: 3.0660 - val_loss: 14.4106 - val_root_mean_squared_error: 3.6097\n",
      "Epoch 39/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1875 - root_mean_squared_error: 3.0414 - val_loss: 14.1082 - val_root_mean_squared_error: 3.5782\n",
      "Epoch 40/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1800 - root_mean_squared_error: 3.0404 - val_loss: 14.3480 - val_root_mean_squared_error: 3.6064\n",
      "Epoch 41/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0793 - root_mean_squared_error: 3.0332 - val_loss: 14.4409 - val_root_mean_squared_error: 3.6106\n",
      "Epoch 42/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1032 - root_mean_squared_error: 3.0399 - val_loss: 14.5108 - val_root_mean_squared_error: 3.6156\n",
      "Epoch 43/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1026 - root_mean_squared_error: 3.0374 - val_loss: 14.7650 - val_root_mean_squared_error: 3.6611\n",
      "Epoch 44/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1284 - root_mean_squared_error: 3.0397 - val_loss: 15.3691 - val_root_mean_squared_error: 3.7481\n",
      "Epoch 45/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1574 - root_mean_squared_error: 3.0476 - val_loss: 14.1711 - val_root_mean_squared_error: 3.5784\n",
      "Epoch 46/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3532 - root_mean_squared_error: 3.0752 - val_loss: 16.0999 - val_root_mean_squared_error: 3.8449\n",
      "Epoch 47/50\n",
      "193536/193536 [==============================] - 5s 25us/step - loss: 10.1512 - root_mean_squared_error: 3.0434 - val_loss: 14.1193 - val_root_mean_squared_error: 3.5730\n",
      "Epoch 48/50\n",
      "193536/193536 [==============================] - 5s 24us/step - loss: 9.9798 - root_mean_squared_error: 3.0170 - val_loss: 14.1385 - val_root_mean_squared_error: 3.5740\n",
      "Epoch 49/50\n",
      "193536/193536 [==============================] - 5s 26us/step - loss: 10.0560 - root_mean_squared_error: 3.0262 - val_loss: 14.9952 - val_root_mean_squared_error: 3.7079\n",
      "Epoch 50/50\n",
      "193536/193536 [==============================] - 5s 27us/step - loss: 10.1102 - root_mean_squared_error: 3.0339 - val_loss: 14.7000 - val_root_mean_squared_error: 3.6638\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "\n",
      "Train on 193536 samples, validate on 48385 samples\n",
      "Epoch 1/50\n",
      "193536/193536 [==============================] - 15s 75us/step - loss: 10.6111 - root_mean_squared_error: 3.1045 - val_loss: 13.7788 - val_root_mean_squared_error: 3.5273\n",
      "Epoch 2/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.4856 - root_mean_squared_error: 3.0910 - val_loss: 14.1891 - val_root_mean_squared_error: 3.5905\n",
      "Epoch 3/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.5365 - root_mean_squared_error: 3.0988 - val_loss: 13.6725 - val_root_mean_squared_error: 3.5190\n",
      "Epoch 4/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.4554 - root_mean_squared_error: 3.0885 - val_loss: 13.6495 - val_root_mean_squared_error: 3.5163\n",
      "Epoch 5/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4843 - root_mean_squared_error: 3.0941 - val_loss: 14.3699 - val_root_mean_squared_error: 3.6182\n",
      "Epoch 6/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.4721 - root_mean_squared_error: 3.0918 - val_loss: 13.7108 - val_root_mean_squared_error: 3.5209\n",
      "Epoch 7/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.4331 - root_mean_squared_error: 3.0865 - val_loss: 13.5555 - val_root_mean_squared_error: 3.5015\n",
      "Epoch 8/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4403 - root_mean_squared_error: 3.0883 - val_loss: 14.0007 - val_root_mean_squared_error: 3.5610\n",
      "Epoch 9/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4300 - root_mean_squared_error: 3.0850 - val_loss: 13.5963 - val_root_mean_squared_error: 3.5117\n",
      "Epoch 10/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4276 - root_mean_squared_error: 3.0801 - val_loss: 13.6403 - val_root_mean_squared_error: 3.5132\n",
      "Epoch 11/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3920 - root_mean_squared_error: 3.0735 - val_loss: 13.7654 - val_root_mean_squared_error: 3.5294\n",
      "Epoch 12/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.3888 - root_mean_squared_error: 3.0772 - val_loss: 13.4950 - val_root_mean_squared_error: 3.4869\n",
      "Epoch 13/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3661 - root_mean_squared_error: 3.0778 - val_loss: 14.1514 - val_root_mean_squared_error: 3.5900\n",
      "Epoch 14/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4074 - root_mean_squared_error: 3.0844 - val_loss: 14.1157 - val_root_mean_squared_error: 3.5815\n",
      "Epoch 15/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4245 - root_mean_squared_error: 3.0857 - val_loss: 13.5191 - val_root_mean_squared_error: 3.4939\n",
      "Epoch 16/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3557 - root_mean_squared_error: 3.0769 - val_loss: 13.6374 - val_root_mean_squared_error: 3.5179\n",
      "Epoch 17/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3491 - root_mean_squared_error: 3.0789 - val_loss: 13.6760 - val_root_mean_squared_error: 3.5126\n",
      "Epoch 18/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3016 - root_mean_squared_error: 3.0640 - val_loss: 13.7698 - val_root_mean_squared_error: 3.5320\n",
      "Epoch 19/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.2675 - root_mean_squared_error: 3.0649 - val_loss: 15.6335 - val_root_mean_squared_error: 3.7932\n",
      "Epoch 20/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2898 - root_mean_squared_error: 3.0658 - val_loss: 13.9268 - val_root_mean_squared_error: 3.5534\n",
      "Epoch 21/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2444 - root_mean_squared_error: 3.0605 - val_loss: 13.8220 - val_root_mean_squared_error: 3.5411\n",
      "Epoch 22/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2910 - root_mean_squared_error: 3.0713 - val_loss: 14.1792 - val_root_mean_squared_error: 3.5893\n",
      "Epoch 23/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.4831 - root_mean_squared_error: 3.0969 - val_loss: 14.1574 - val_root_mean_squared_error: 3.5836\n",
      "Epoch 24/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2484 - root_mean_squared_error: 3.0618 - val_loss: 13.6547 - val_root_mean_squared_error: 3.5104\n",
      "Epoch 25/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2405 - root_mean_squared_error: 3.0550 - val_loss: 14.1187 - val_root_mean_squared_error: 3.5831\n",
      "Epoch 26/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.2343 - root_mean_squared_error: 3.0554 - val_loss: 13.9880 - val_root_mean_squared_error: 3.5658\n",
      "Epoch 27/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.5169 - root_mean_squared_error: 3.0892 - val_loss: 14.5793 - val_root_mean_squared_error: 3.6471\n",
      "Epoch 28/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.1375 - root_mean_squared_error: 3.0346 - val_loss: 13.6210 - val_root_mean_squared_error: 3.5087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1822 - root_mean_squared_error: 3.0487 - val_loss: 13.7180 - val_root_mean_squared_error: 3.5251\n",
      "Epoch 30/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1431 - root_mean_squared_error: 3.0436 - val_loss: 13.7444 - val_root_mean_squared_error: 3.5240\n",
      "Epoch 31/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1916 - root_mean_squared_error: 3.0498 - val_loss: 14.1372 - val_root_mean_squared_error: 3.5756\n",
      "Epoch 32/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1754 - root_mean_squared_error: 3.0484 - val_loss: 14.5730 - val_root_mean_squared_error: 3.6386\n",
      "Epoch 33/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1560 - root_mean_squared_error: 3.0482 - val_loss: 14.6996 - val_root_mean_squared_error: 3.6549\n",
      "Epoch 34/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1151 - root_mean_squared_error: 3.0383 - val_loss: 13.6432 - val_root_mean_squared_error: 3.5100\n",
      "Epoch 35/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.3283 - root_mean_squared_error: 3.0666 - val_loss: 14.5000 - val_root_mean_squared_error: 3.6180\n",
      "Epoch 36/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1269 - root_mean_squared_error: 3.0377 - val_loss: 14.0227 - val_root_mean_squared_error: 3.5521\n",
      "Epoch 37/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1534 - root_mean_squared_error: 3.0451 - val_loss: 14.7964 - val_root_mean_squared_error: 3.6673\n",
      "Epoch 38/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1342 - root_mean_squared_error: 3.0420 - val_loss: 14.3461 - val_root_mean_squared_error: 3.6063\n",
      "Epoch 39/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0761 - root_mean_squared_error: 3.0385 - val_loss: 13.8574 - val_root_mean_squared_error: 3.5312\n",
      "Epoch 40/50\n",
      "193536/193536 [==============================] - 4s 22us/step - loss: 10.0596 - root_mean_squared_error: 3.0288 - val_loss: 13.7623 - val_root_mean_squared_error: 3.5245\n",
      "Epoch 41/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0512 - root_mean_squared_error: 3.0316 - val_loss: 13.8866 - val_root_mean_squared_error: 3.5434\n",
      "Epoch 42/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0801 - root_mean_squared_error: 3.0340 - val_loss: 14.1155 - val_root_mean_squared_error: 3.5643\n",
      "Epoch 43/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0513 - root_mean_squared_error: 3.0314 - val_loss: 14.0556 - val_root_mean_squared_error: 3.5652\n",
      "Epoch 44/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.1644 - root_mean_squared_error: 3.0508 - val_loss: 38.1556 - val_root_mean_squared_error: 6.0469\n",
      "Epoch 45/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.2348 - root_mean_squared_error: 3.0544 - val_loss: 14.0482 - val_root_mean_squared_error: 3.5632\n",
      "Epoch 46/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0894 - root_mean_squared_error: 3.0303 - val_loss: 13.7267 - val_root_mean_squared_error: 3.5137\n",
      "Epoch 47/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0376 - root_mean_squared_error: 3.0336 - val_loss: 13.6894 - val_root_mean_squared_error: 3.5069\n",
      "Epoch 48/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0052 - root_mean_squared_error: 3.0223 - val_loss: 14.0281 - val_root_mean_squared_error: 3.5557\n",
      "Epoch 49/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0962 - root_mean_squared_error: 3.0385 - val_loss: 13.9714 - val_root_mean_squared_error: 3.5517\n",
      "Epoch 50/50\n",
      "193536/193536 [==============================] - 4s 21us/step - loss: 10.0218 - root_mean_squared_error: 3.0291 - val_loss: 14.3892 - val_root_mean_squared_error: 3.6083\n"
     ]
    }
   ],
   "source": [
    "best_loss = [0] * 4\n",
    "learn_list = [0.001, 0.00025, 0.00015, 0.0001]\n",
    "\n",
    "for i in [0, 1, 2, 3]:\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = learn_list[i]\n",
    "    REGULARIZATION_RATE = 0.01\n",
    "    make_new_model = True\n",
    "\n",
    "    print('\\nLearning Rate: {}\\n'.format(LEARNING_RATE))\n",
    "    \n",
    "    callback = [#EarlyStopping(patience=60, monitor='val_loss'),\n",
    "                #ReduceLROnPlateau(patience=20, monitor='val_loss', factor=0.5, min_lr=0.00001, verbose=1),\n",
    "                #LearningRateScheduler(learning_scheduale),\n",
    "                ModelCheckpoint('model{}'.format(i), monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    if make_new_model:\n",
    "        model = make_model(LEARNING_RATE, REGULARIZATION_RATE, [256] * 6, 0)\n",
    "        \n",
    "        if i > 0:\n",
    "            model.load_weights('model{}'.format(i-1))\n",
    "\n",
    "    history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                        verbose=1, validation_data=(validation_df_scaled, validation_labels), \n",
    "                        shuffle=True, callbacks = callback)\n",
    "    \n",
    "    best_loss[i] = min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.825783317453965\n",
      "13.55271577426098\n",
      "13.534215311724699\n",
      "13.495046053921202\n"
     ]
    }
   ],
   "source": [
    "for loss in best_loss:\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 0.0001\n",
    "REGULARIZATION_RATE = 0.001\n",
    "make_new_model = True\n",
    "\n",
    "callback = [#EarlyStopping(patience=60, monitor='val_loss'),\n",
    "            #ReduceLROnPlateau(patience=20, monitor='val_loss', factor=0.5, min_lr=0.00001, verbose=1),\n",
    "            LearningRateScheduler(learning_scheduale),\n",
    "            ModelCheckpoint('model', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "# 256, 256, 128, 64 is best at 3.34\n",
    "\n",
    "if make_new_model:\n",
    "    model = make_model(LEARNING_RATE, REGULARIZATION_RATE, [256,256,128,128,64,64])\n",
    "\n",
    "model.load_weights('model320')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list = [0] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind = 0\n",
    "\n",
    "for i in [1,0.1,0.01,0.001,0.0001]:\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    REGULARIZATION_RATE = i\n",
    "    make_new_model = True\n",
    "\n",
    "    callback = [#EarlyStopping(patience=60, monitor='val_loss'),\n",
    "                #ReduceLROnPlateau(patience=20, monitor='val_loss', factor=0.5, min_lr=0.00001, verbose=1),\n",
    "                LearningRateScheduler(learning_scheduale),\n",
    "                ModelCheckpoint('model', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    # 256, 256, 128, 64 is best at 3.34\n",
    "\n",
    "    if make_new_model:\n",
    "        model = make_model(LEARNING_RATE, REGULARIZATION_RATE, [256] * 6, 0)\n",
    "\n",
    "    history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                        verbose=1, validation_data=(validation_df_scaled, validation_labels), \n",
    "                        shuffle=True, callbacks = callback)\n",
    "\n",
    "    history_list[ind] = history\n",
    "    \n",
    "    ind += 1\n",
    "    \n",
    "model.save_weights('model_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_list(history_list):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for history in history_list:\n",
    "        plt.plot(history.history['val_loss'])\n",
    "    plt.title('Differing Hidden Layer Size', fontsize=24)\n",
    "    plt.ylabel('validation loss', fontsize=24)\n",
    "    plt.xlabel('epoch', fontsize=24)\n",
    "    plt.legend(['32 nodes', '64 nodes', '128 nodes',\n",
    "               '256 nodes', '512 nodes'], loc='upper right', fontsize=24)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    for history in history_list:\n",
    "        plt.plot(history.history['val_root_mean_squared_error'])\n",
    "    plt.title('Differing Hidden Layer Size RMSE')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['1.0', '0.1', '0.01',\n",
    "               '0.001', '0.0001'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_list(history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in history_list:\n",
    "    print(min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in history_list:\n",
    "    print(min(history.history['val_root_mean_squared_error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('train.csv', nrows=5000000, dtype=datatypes, usecols=[1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.sample(frac=0.1)\n",
    "\n",
    "temp.to_csv('500k_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
